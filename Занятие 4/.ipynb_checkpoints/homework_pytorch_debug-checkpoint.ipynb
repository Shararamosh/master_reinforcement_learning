{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqZ2EwnTZdC8"
   },
   "source": [
    "# Deep Q-Network implementation.\n",
    "\n",
    "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
    "\n",
    "Original paper:\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv7XJfXaZdC9"
   },
   "source": [
    "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
    "\n",
    "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
    "\n",
    "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can get 1 pt (instead of 3 pts) for beating the threshold in debug notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioIEVODJZdC9",
    "outputId": "871bc6a2-d9eb-403d-a1b8-45006cd349b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "(Reading database ... 120874 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libxfont2:amd64.\n",
      "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
      "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
      "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Selecting previously unselected package x11-xkb-utils.\n",
      "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
      "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
      "Selecting previously unselected package xfonts-encodings.\n",
      "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
      "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Selecting previously unselected package xfonts-utils.\n",
      "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
      "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
      "Selecting previously unselected package xfonts-base.\n",
      "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
      "Unpacking xfonts-base (1:1.0.5) ...\n",
      "Selecting previously unselected package xserver-common.\n",
      "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
      "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Selecting previously unselected package xvfb.\n",
      "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
      "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Setting up x11-xkb-utils (7.7+5build4) ...\n",
      "Setting up xfonts-utils (1:7.7+6build2) ...\n",
      "Setting up xfonts-base (1:1.0.5) ...\n",
      "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "Starting virtual X frame buffer: Xvfb.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8OFQOtGojc8",
    "outputId": "1db7b5e6-081b-4589-8ad2-490f28b8bcb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/953.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDZqlI3kZdC9"
   },
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dsYq558wZdC-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6ypPZ8e6ZdC-"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j8EGNlSZdC-"
   },
   "source": [
    "### CartPole again\n",
    "\n",
    "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
    "\n",
    "CartPole is the simplest one. It should take several minutes to solve it.\n",
    "\n",
    "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v-5u-CcQZdC-"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "AmFXRrkqZdC-",
    "outputId": "7169e4e4-dd2c-42cb-c519-184a8caab7a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOyWgOmvZdC-"
   },
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqpThLZXZdC-"
   },
   "source": [
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVlpkvZOZdC-",
    "outputId": "03c82433-7b09-4c20-b0ef-758b8b2bfe3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RFva1cpyZdC-"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        self.network = nn.Sequential()\n",
    "        self.network.add_module('dense1', nn.Linear(in_features =  state_dim, out_features = 128))\n",
    "        self.network.add_module('dense1_relu', nn.ReLU())\n",
    "        self.network.add_module('dense2', nn.Linear(in_features =  128, out_features = n_actions))\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = self.network(state_t)\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and\n",
    "            qvalues.shape[0] == state_t.shape[0] and\n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Bv1s5JKzZdC-"
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vazC0DPQZdC_"
   },
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e-Sg1cqPZdC_"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset(seed=seed)\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_0NzjUEZdC_"
   },
   "source": [
    "### Experience replay\n",
    "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHyCO4TuZdC_"
   },
   "source": [
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lmBw-UG_FmRr"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "\n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "\n",
    "        # OPTIONAL: YOUR CODE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize.\n",
    "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        # add data to storage\n",
    "        while len(self._storage) >= self._maxsize:\n",
    "          self._storage.pop(0)\n",
    "        self._storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        if len(self._storage) < 2:\n",
    "          idxes = np.array([0]*batch_size)\n",
    "        else:\n",
    "          idxes = np.random.randint(len(self._storage), size = batch_size)\n",
    "        obs_batch = np.array(list(zip(*self._storage))[0])[idxes]\n",
    "        act_batch = np.array(list(zip(*self._storage))[1])[idxes]\n",
    "        rew_batch = np.array(list(zip(*self._storage))[2])[idxes]\n",
    "        next_obs_batch = np.array(list(zip(*self._storage))[3])[idxes]\n",
    "        done_mask = np.array(list(zip(*self._storage))[4])[idxes]\n",
    "        # collect <s,a,r,s',done> for each index\n",
    "        return (obs_batch, act_batch, rew_batch, next_obs_batch, done_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wQEHwR1AZdC_"
   },
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0RnFX5sfZdC_"
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
    "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
    "    It is guaranteed that env has terminated=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for i in range(n_steps):\n",
    "      action = agent.sample_actions(agent.get_qvalues([s]))[0]\n",
    "      next_s, r, terminated, truncated, _ = env.step(action)\n",
    "      sum_rewards += r\n",
    "      if exp_replay is not None:\n",
    "        exp_replay.add(s, action, r, next_s, terminated)\n",
    "      s = next_s\n",
    "      if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXXmFEKGZdC_",
    "outputId": "f0a18db4-7398-47bf-b04d-a5667f054db1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2112362e7536>:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  states = torch.tensor(states, device=model_device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
    "    \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1, \\\n",
    "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
    "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
    "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
    "        np.mean(is_dones), len(exp_replay))\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (10,), \\\n",
    "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done should be strictly True or False\"\n",
    "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoVGsnHRZdC_"
   },
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BLJCNiuZdC_",
    "outputId": "b2d54b6f-2b64-44ac-d707-83d3b67e8cbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
